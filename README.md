# `@mux/ai`   

[![npm version](https://badge.fury.io/js/@mux%2Fai.svg)](https://www.npmjs.com/package/@mux/ai)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

> Build production-ready video and audio AI features in TypeScript using your existing [Mux](https://www.mux.com) assets.

`@mux/ai` turns Mux video data (captions, transcripts, thumbnails, and storyboards) into AI-ready inputs so you can ship features faster without wiring up complex media and prompting pipelines yourself.

## Why teams integrate `@mux/ai`

- **Ship faster:** Use typed, pre-built workflows for common product features (summaries, moderation, chapters, translation, dubbing, embeddings).
- **Stay flexible on providers:** Use OpenAI, Anthropic, Google, Hive, or ElevenLabs depending on workflow and cost/quality goals.
- **Build on Mux-native context:** Workflows pull the right media context from Mux automatically.
- **Scale with safer defaults:** Includes retries, structured outputs, and battle-tested workflow patterns.

## Quick start

### Prerequisites

- [Node.js](https://nodejs.org/en/download) (>= 21.0.0)
- A Mux account and API credentials (create them [here](https://dashboard.mux.com/settings/access-tokens))
- API keys for any AI providers you want to use

### Install

```bash
npm install @mux/ai
```

### Configure credentials

`@mux/ai` supports [dotenv](https://www.npmjs.com/package/dotenv), so a `.env` file works out of the box:

```bash
# Required for all workflows
MUX_TOKEN_ID=your_mux_token_id
MUX_TOKEN_SECRET=your_mux_token_secret

# Example provider key (configure only what you use)
OPENAI_API_KEY=your_openai_api_key
```

> If your assets use signed playback IDs only, also set `MUX_SIGNING_KEY` and `MUX_PRIVATE_KEY`.

### Run your first workflow

```ts
import { getSummaryAndTags } from "@mux/ai/workflows";

const result = await getSummaryAndTags("your-asset-id", {
  provider: "openai",
  includeTranscript: true,
  tone: "professional",
});

console.log(result.title);
console.log(result.description);
console.log(result.tags);
```

> Many workflows are best with transcripts. Enable [auto-generated captions](https://www.mux.com/docs/guides/add-autogenerated-captions-and-use-transcripts) on assets for better summarization, chapters, and embeddings.

## Workflow catalog

| Workflow | Good for | Providers | Asset requirements |
| --- | --- | --- | --- |
| [`getSummaryAndTags`](./docs/WORKFLOWS.md#video-summarization)<br/>[API](./docs/API.md#getsummaryandtagsassetid-options) 路 [Source](./src/workflows/summarization.ts) | Titles, descriptions, tags | OpenAI, Anthropic, Google | Video or audio-only; transcript recommended |
| [`getModerationScores`](./docs/WORKFLOWS.md#content-moderation)<br/>[API](./docs/API.md#getmoderationscoresassetid-options) 路 [Source](./src/workflows/moderation.ts) | Sexual/violence moderation scores | OpenAI, Hive | Video, or audio-only with transcript (OpenAI) |
| [`hasBurnedInCaptions`](./docs/WORKFLOWS.md#burned-in-caption-detection)<br/>[API](./docs/API.md#hasburnedincaptionsassetid-options) 路 [Source](./src/workflows/burned-in-captions.ts) | Detect hardcoded subtitles | OpenAI, Anthropic, Google | Video required |
| [`askQuestions`](./docs/WORKFLOWS.md#ask-questions)<br/>[API](./docs/API.md#askquestionsassetid-questions-options) 路 [Source](./src/workflows/ask-questions.ts) | Yes/no Q&A over asset content | OpenAI, Anthropic, Google | Video; transcript optional |
| [`generateChapters`](./docs/WORKFLOWS.md#chapter-generation)<br/>[API](./docs/API.md#generatechaptersassetid-languagecode-options) 路 [Source](./src/workflows/chapters.ts) | Time-based chapters | OpenAI, Anthropic, Google | Transcript required |
| [`generateEmbeddings`](./docs/WORKFLOWS.md#embeddings)<br/>[API](./docs/API.md#generateembeddingsassetid-options) 路 [Source](./src/workflows/embeddings.ts) | Semantic search embeddings | OpenAI, Google | Transcript required |
| [`translateCaptions`](./docs/WORKFLOWS.md#caption-translation)<br/>[API](./docs/API.md#translatecaptionsassetid-fromlanguagecode-tolanguagecode-options) 路 [Source](./src/workflows/translate-captions.ts) | Caption translation | OpenAI, Anthropic, Google | Transcript required; S3 needed if uploading back to Mux |
| [`translateAudio`](./docs/WORKFLOWS.md#audio-dubbing)<br/>[API](./docs/API.md#translateaudioassetid-tolanguagecode-options) 路 [Source](./src/workflows/translate-audio.ts) | AI dubbing for localized audio tracks | ElevenLabs | Audio required; S3 needed if uploading back to Mux |

## Workflows vs. primitives

Use **workflows** when you want end-to-end functionality quickly:

```ts
import { generateChapters } from "@mux/ai/workflows";

const result = await generateChapters("asset-id", "en", { provider: "openai" });
```

Use **primitives** when you want complete control over prompts and orchestration:

```ts
import { chunkByTokens, getStoryboardUrl } from "@mux/ai/primitives";

const storyboardUrl = await getStoryboardUrl("playback-id", 640);
const chunks = chunkByTokens("Long transcript text...", 500, 100);
```

## Workflow DevKit compatibility

All exported workflows are compatible with [Workflow DevKit](https://useworkflow.dev) via `"use workflow"` directives.

```ts
import { start } from "workflow/api";
import { getSummaryAndTags } from "@mux/ai/workflows";

const run = await start(getSummaryAndTags, ["your-asset-id"]);
```

For secure, multi-tenant credential handling, read:
- [Workflow Encryption guide](./docs/WORKFLOW-ENCRYPTION.md)

## Credentials at a glance

| Scenario | Environment variables |
| --- | --- |
| Required for all workflows | `MUX_TOKEN_ID`, `MUX_TOKEN_SECRET` |
| Signed playback only | `MUX_SIGNING_KEY`, `MUX_PRIVATE_KEY` |
| OpenAI workflows | `OPENAI_API_KEY` |
| Anthropic workflows | `ANTHROPIC_API_KEY` |
| Google workflows | `GOOGLE_GENERATIVE_AI_API_KEY` |
| Hive moderation | `HIVE_API_KEY` |
| ElevenLabs dubbing | `ELEVENLABS_API_KEY` |
| Translation/dubbing upload to Mux (`uploadToMux=true`) | `S3_ENDPOINT`, `S3_REGION`, `S3_BUCKET`, `S3_ACCESS_KEY_ID`, `S3_SECRET_ACCESS_KEY` |

## Documentation map

Use this as your "start here" guide based on what you're building:

- **Pre-built workflow behavior and options:** [Workflows Guide](./docs/WORKFLOWS.md)
- **Function signatures and return shapes:** [API Reference](./docs/API.md)
- **Custom pipelines with building blocks:** [Primitives Guide](./docs/PRIMITIVES.md)
- **Copy/paste runnable examples:** [Examples](./docs/EXAMPLES.md)
- **Audio-only asset support details:** [Audio-Only Workflows](./docs/AUDIO-ONLY.md)
- **Custom storage integrations (AWS/R2/MinIO):** [Storage Adapters](./docs/STORAGE-ADAPTERS.md)
- **Workflow DevKit secret handling:** [Workflow Encryption](./docs/WORKFLOW-ENCRYPTION.md)
- **Quality/cost benchmarking and eval framework:** [Evaluations](./docs/EVALS.md)

## Additional resources

- [Mux Video API Docs](https://docs.mux.com/guides/video)
- [Auto-generated captions guide](https://www.mux.com/docs/guides/add-autogenerated-captions-and-use-transcripts)
- [GitHub repository](https://github.com/muxinc/ai)
- [npm package](https://www.npmjs.com/package/@mux/ai)

## Contributing

Contributions are welcome. See [CONTRIBUTING.md](./CONTRIBUTING.md) for development setup, test commands, and contribution guidelines.

> Integration tests run against real Mux assets. If you plan to run `npm run test:integration`, also configure the required Mux test asset IDs (see `env.test.example` and `CONTRIBUTING.md`).

## License

[Apache 2.0](LICENSE)
